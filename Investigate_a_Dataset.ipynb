{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Finding Correlations among Unrelated Variables\n",
    "\n",
    "## Table of Contents\n",
    "<ul>\n",
    "<li><a href=\"#intro\">Introduction</a></li>\n",
    "<li><a href=\"#wrangling\">Data Wrangling</a></li>\n",
    "<li><a href=\"#eda\">Exploring the Data</a></li>\n",
    "<li><a href=\"#conclusions\">Conclusions</a></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='intro'></a>\n",
    "## I) Introduction\n",
    "\n",
    "**Broad question:** How do total forest area and frequency of natural disasters shape a country's obesity rates and murder rates?\n",
    "\n",
    "> I picked these factors which seem to be unrelated--geography & frequency of natural disasters, to rates of obesity murder, to formulate new interesting questions and uncover unexpected patterns. I also wanted to approach this project through an experimental and free-for-all lens, just to see if I can make any fun or comical conclusions from the giving unrelated datasets. To narrow down my focus, I picked subcategory of [TK] for geography, [TK] for education and [TK] to encompass murder rates. I received all of my data through GapMinder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='wrangling'></a>\n",
    "# II) Data Wrangling\n",
    "\n",
    "## A) Gathering Data\n",
    "\n",
    "#### Natural Disasters\n",
    "> I took [TK list dataset names here], each depicting the number of deaths of their respective natural disaster. To combine those datasets, I generated a new CSV file named ‘natural_disaster_deaths.csv’, which depicts the sum of deaths by natural disaster per year for each country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load all DataFrames for natural disasters\n",
    "filepath_disasters = './data/natural_disasters/'\n",
    "\n",
    "#assign country names as primary indexes\n",
    "df_drought = pd.read_csv(filepath_disasters + 'indicator_drought_killed.csv', index_col = 'Drought killed')\n",
    "df_earthquake = pd.read_csv(filepath_disasters + 'indicator_earthquake_killed.csv', index_col = 'Earthquake killed')\n",
    "df_epidemic = pd.read_csv(filepath_disasters + 'indicator_epidemic_killed.csv', index_col = 'Epidemic killed')\n",
    "df_flood = pd.read_csv(filepath_disasters + 'indicator_flood_killed.csv', index_col = 'Flood killed')\n",
    "df_storm = pd.read_csv(filepath_disasters + 'indicator_storm_killed.csv', index_col = 'Storm killed')\n",
    "df_tsunami = pd.read_csv(filepath_disasters + 'indicator_tsunami_killed.csv', index_col = 'Tsunami killed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "#put all DataFrames in a NumPy array for more efficient handling later\n",
    "dfs_nd = np.array([df_drought, df_earthquake, df_epidemic, df_flood, df_storm, df_tsunami])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Total Forest Area\n",
    "> I am focusing on total natural forest land per country, and so I will exclude factors that indicate if said forest land is reserved for agricultural production.\n",
    "\n",
    "> From GapMinder, forest area is described as ‘land under natural or planted stands of trees of at least 5 meters in situ, whether productive or not, and excludes tree stands in agricultural production systems (for example, in fruit plantations and agroforestry systems) and trees in urban parks and gardens.' The dataset ‘forest_area_sq_km.csv’ keeps track of the total forest area from 1990 to 2015."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_forest = pd.read_csv('./data/forest_area_sq_km.csv', index_col='country')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Obesity Rates\n",
    "> GapMinder provided the age standardized mean for BMI, dividing it into BMI values for men and women. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bmi_male = pd.read_csv('./data/bmi_rates/bmi_male.csv', index_col='Country')\n",
    "df_bmi_female = pd.read_csv('./data/bmi_rates/bmi_female.csv', index_col='Country')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Murder Rates\n",
    "> Encompasses number of murders per 100,000 people, accounting for all ages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_murder = pd.read_csv('./data/homicide_rates.csv', index_col='Murder per 100,000, age adjusted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B) Data Cleaning\n",
    "\n",
    "### Natural Disasters\n",
    "> To account for all natural disasters that occurred in each country, my goal is to generate a new CSV file where each cell contains the sum of all the natural disaster DataFrames.\n",
    "\n",
    "Let's examine the columns of each natural disaster DataFrame. There are 195 total countries in the world, so I am expecting there to be at most 195 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drought killed (128, 39)\n",
      "Earthquake killed (97, 39)\n",
      "Epidemic killed (143, 38)\n",
      "Flood killed (182, 39)\n",
      "Storm killed (181, 39)\n",
      "Tsunami killed (18, 15)\n"
     ]
    }
   ],
   "source": [
    "for df in np.ndenumerate(dfs_nd):\n",
    "    #display (row, column) per DataFrame\n",
    "    print(df[1].index.name , df[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> It can be seen that the DataFrame for tsunamis is the least reported out of all the other natural disasters (accounting for only 18 countries), while the most documented natural disaster is floods, at 181 countries. This adds more ambiguity as to how we should generate our final CSV file accounting for all natural disasters in all countries.\n",
    "\n",
    "> Now there are two possibilities for approaching this:\n",
    "\n",
    "Approach 1) The missing countries means that no natural disasters occurred in them, and so it was not necessary to include them in their respective DataFrames.\n",
    "\n",
    "> Therefore **it is safe to set the values of the missing countries to 0 and generate the final CSV as a sum of all the DataFrames.**\n",
    "\n",
    "Approach 2) We do not know if any natural disasters occurred in the missing countries.\n",
    "\n",
    "> Therefore **we should focus only the countries in common who have no missing values.**\n",
    "\n",
    "I ended up picking **Approach 2** because as can be seen in the above df_drought DataFrame, there are values for 0 in countries and years where no earthquakes happened. The best conclusion I can come for the missing indexed countries is that there has been no data recorded for them, and therefore I cannot assume whether or not earthquakes ever occurred in the missing countries. This logic extends to the rest of the DataFrames for the natural disasters.\n",
    "\n",
    "**But before we go limiting countries, let's rename the indexed countries as 'country' to allow for more efficient indexing.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     1970  1971  1972  1973  1974  1975  1976  1977  1978  \\\n",
      "country                                                                     \n",
      "Afghanistan             0     0     0     0     0     0     0     0     0   \n",
      "Albania                 0     0     0     0     0     0     0     0     0   \n",
      "Algeria                 0     0     0     0     0     0     0     0     0   \n",
      "Angola                  0     0     0     0     0     0     0     0     0   \n",
      "Antigua and Barbuda     0     0     0     0     0     0     0     0     0   \n",
      "\n",
      "                     1979  ...  1999  2000  2001  2002  2003  2004  2005  \\\n",
      "country                    ...                                             \n",
      "Afghanistan             0  ...     0    37     0     0     0     0     0   \n",
      "Albania                 0  ...     0     0     0     0     0     0     0   \n",
      "Algeria                 0  ...     0     0     0     0     0     0    12   \n",
      "Angola                  0  ...     0     0    58     0     0     0     0   \n",
      "Antigua and Barbuda     0  ...     0     0     0     0     0     0     0   \n",
      "\n",
      "                     2006  2007  2008  \n",
      "country                                \n",
      "Afghanistan             0     0     0  \n",
      "Albania                 0     0     0  \n",
      "Algeria                 0     0     0  \n",
      "Angola                  0     0     0  \n",
      "Antigua and Barbuda     0     0     0  \n",
      "\n",
      "[5 rows x 39 columns]\n",
      "                1970  1971  1972  1973  1974  1975  1976  1977  1978  1979  \\\n",
      "country                                                                      \n",
      "Afghanistan        0     0     0     0     0     0    50     0     0     0   \n",
      "Albania            0     0     0     0     0     0     0     0     0    35   \n",
      "Algeria            0     0     0     0     0     0     0     0     0     0   \n",
      "American Samoa     0     0     0     0     0     0     0     0     0     0   \n",
      "Argentina          0     0     0     0     0     0     0    70     0     0   \n",
      "\n",
      "                ...  1999  2000  2001  2002  2003  2004  2005  2006  2007  \\\n",
      "country         ...                                                         \n",
      "Afghanistan     ...    70     0     4  1200     1     2     6     1     0   \n",
      "Albania         ...     0     0     0     0     0     0     0     0     0   \n",
      "Algeria         ...    22     0     0     0  2275     0     0     4     0   \n",
      "American Samoa  ...     0     0     0     0     0     0     0     0     0   \n",
      "Argentina       ...     0     0     0     0     0     0     0     0     0   \n",
      "\n",
      "                2008  \n",
      "country               \n",
      "Afghanistan        0  \n",
      "Albania            0  \n",
      "Algeria            0  \n",
      "American Samoa     0  \n",
      "Argentina          0  \n",
      "\n",
      "[5 rows x 39 columns]\n",
      "             1970  1971  1972  1974  1975  1976  1977  1978  1979  1980  ...  \\\n",
      "country                                                                  ...   \n",
      "Afghanistan     0     0     0     0     0     0     0     0     0     0  ...   \n",
      "Albania         0     0     0     0     0     0     0     0     0     0  ...   \n",
      "Algeria         0     0     0     0     0     0     0     0     0     0  ...   \n",
      "Angola          0     0     0     0     0     0     0     0     0     0  ...   \n",
      "Argentina       0     0     0     0     0     0     0     0     0     0  ...   \n",
      "\n",
      "             1999  2000  2001  2002  2003  2004  2005  2006  2007  2008  \n",
      "country                                                                  \n",
      "Afghanistan   135   557   154  2797     0     0     0     0     0    17  \n",
      "Albania         0     0     0     0     0     0     0     0     0     0  \n",
      "Algeria         0     0     0     0     0     0     0     0     0     0  \n",
      "Angola        188    18    39     0     0   329     0  2422   515   229  \n",
      "Argentina       0     0     0     0     0     0     0     0     0     0  \n",
      "\n",
      "[5 rows x 38 columns]\n",
      "                1970  1971  1972  1973  1974  1975  1976  1977  1978  1979  \\\n",
      "country                                                                      \n",
      "Afghanistan        0     0   150     0     0     0    51     0   120     0   \n",
      "Albania            0     0     0     0     0     0     0     0     0     0   \n",
      "Algeria            0     0     0    21    11     0     0     0     0     0   \n",
      "American Samoa     0     0     0     0     0     0     0     0     0     0   \n",
      "Angola             0     0     0     0     0     0     0     0     0     0   \n",
      "\n",
      "                ...  1999  2000  2001  2002  2003  2004  2005  2006  2007  \\\n",
      "country         ...                                                         \n",
      "Afghanistan     ...     0     0     0    81   136    16   295   282   296   \n",
      "Albania         ...     0     0     0     1     0     0     3     0     0   \n",
      "Algeria         ...    12    44   921    48    41    22    12     1    82   \n",
      "American Samoa  ...     0     0     0     0     6     0     0     0     0   \n",
      "Angola          ...     0    46    57     8    15    28     0     0   105   \n",
      "\n",
      "                2008  \n",
      "country               \n",
      "Afghanistan        0  \n",
      "Albania            0  \n",
      "Algeria           89  \n",
      "American Samoa     0  \n",
      "Angola             7  \n",
      "\n",
      "[5 rows x 39 columns]\n",
      "                1970  1971  1972  1973  1974  1975  1976  1977  1978  1979  \\\n",
      "country                                                                      \n",
      "Afghanistan        0     0     0     0     0     0     0     0     0     0   \n",
      "Albania            0     0     0     0     0     0     0     0     0     0   \n",
      "Algeria            0     0     0     0     0     0     0     0     0     0   \n",
      "American Samoa     0     0     0     0     0     0     0     0     0     0   \n",
      "Anguilla           0     0     0     0     0     0     0     0     0     0   \n",
      "\n",
      "                ...  1999  2000  2001  2002  2003  2004  2005  2006  2007  \\\n",
      "country         ...                                                         \n",
      "Afghanistan     ...     0     0     0     0     0     0   260    71     0   \n",
      "Albania         ...     0     0     0     6     0     0     2     0     0   \n",
      "Algeria         ...     0     4     0     0    13     0    10     0     0   \n",
      "American Samoa  ...     0     0     0     0     0     0     0     0     0   \n",
      "Anguilla        ...     0     0     0     0     0     0     0     0     0   \n",
      "\n",
      "                2008  \n",
      "country               \n",
      "Afghanistan        0  \n",
      "Albania            0  \n",
      "Algeria            0  \n",
      "American Samoa     0  \n",
      "Anguilla           0  \n",
      "\n",
      "[5 rows x 39 columns]\n",
      "             1979  1980  1995  1996  1997  1998  1999  2000  2001  2002  2003  \\\n",
      "country                                                                         \n",
      "Bangladesh    NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
      "France       11.0   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
      "Honduras      NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
      "India         NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
      "Indonesia   539.0   0.0   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   0.0   \n",
      "\n",
      "                2004  2005   2006  2007  \n",
      "country                                  \n",
      "Bangladesh       2.0   NaN    NaN   NaN  \n",
      "France           NaN   NaN    NaN   NaN  \n",
      "Honduras         NaN   NaN    NaN   NaN  \n",
      "India        16389.0   NaN    NaN   NaN  \n",
      "Indonesia   165708.0   0.0  802.0   NaN  \n"
     ]
    }
   ],
   "source": [
    "#rename indexed country header as 'country'\n",
    "for df in np.ndenumerate(dfs_nd):\n",
    "    df[1].index.name = 'country'\n",
    "    \n",
    "    #check that the name change did occur\n",
    "    print(df[1].head())\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The above results show that only the last DataFrame for tsunamis contains null values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *DROP NULLS*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "248\n"
     ]
    }
   ],
   "source": [
    "#Display number of NaN values in each dataset\n",
    "for df in np.ndenumerate(dfs_nd):\n",
    "    print(df[1].isna().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop nulls from tsunami dataset\n",
    "df_tsunami.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Confirm changes\n",
    "df_tsunami.isna().any().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**But hold up!**\n",
    "> Before we move on, let's check the first few rows of the tsunami data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1979</th>\n",
       "      <th>1980</th>\n",
       "      <th>1995</th>\n",
       "      <th>1996</th>\n",
       "      <th>1997</th>\n",
       "      <th>1998</th>\n",
       "      <th>1999</th>\n",
       "      <th>2000</th>\n",
       "      <th>2001</th>\n",
       "      <th>2002</th>\n",
       "      <th>2003</th>\n",
       "      <th>2004</th>\n",
       "      <th>2005</th>\n",
       "      <th>2006</th>\n",
       "      <th>2007</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>country</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [1979, 1980, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007]\n",
       "Index: []"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check how many values exist in the tsunami data set\n",
    "print(df_tsunami.count().sum())\n",
    "\n",
    "df_tsunami"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The tsunami dataset is completely empty, which means we'll have to discard it. This makes me come to the conclusion that ***every single row in the data set contained a null value, which means that after dropping all the null values, the dataset became completely empty.***\n",
    "\n",
    "> I know it's painful to have to discard an entire DataFrame, but at least that is better than incorporating largely unreliable data into our final analysis.\n",
    "\n",
    "Before moving on, let's create a new NumPy array of the DataFrames without the tsunami dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Drop tsunami dataset\n",
    "dfs_nd_dropped = np.delete(dfs_nd, -1)\n",
    "\n",
    "#Check changes\n",
    "len(dfs_nd_dropped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are only 5 DataFrames in the NumPy array instead of 6, so we can move on.\n",
    "\n",
    "### *Dedupe Data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101\n",
      "21\n",
      "21\n",
      "30\n",
      "29\n"
     ]
    }
   ],
   "source": [
    "#Check for number of duplicate data per row\n",
    "for df in np.ndenumerate(dfs_nd_dropped):\n",
    "    print(df[1].duplicated().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> But wait, we don't want to drop duplicate data just yet. As we know, many of the DataFrames are populated with 0's. Before we drop duplicate rows, let's make sure that they are all actually **zeroes** and not just rows populated with the same repeating values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "#Display all duplicated rows in each DataFrame\n",
    "for df in np.ndenumerate(dfs_nd_dropped):\n",
    "    #return True if the dataset contains a value that's not 0\n",
    "    print(df[1][df[1].duplicated(keep=False)].any().any())\n",
    "    \n",
    "    \n",
    "# print(df[1][df[1].duplicated(keep=False)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Some of the DataFrames have values that are greater than 0. But I've come to realize I don't necessarily care about the duplicate values themselves. The real question is: **are they repeating countries?** \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "#Check if any of the DataFrames have duplicate index 'country'\n",
    "for df in np.ndenumerate(dfs_nd_dropped):\n",
    "    #return True if the dataset contains a repeating country\n",
    "    print(df[1].index.duplicated().sum().any())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> None of the DataFrames have repeating countries for indexes. This is good news! Our data has turned out to be much more reliable than expected.\n",
    "\n",
    "Now we can decide to drop the duplicated rows or not, but I have **ultimately decided not to.** This is an exceptional case. Dropping the rows could harm our data reliability in the end, because dropping duplicates means we would be getting rid of natural disaster scores for entire countries in the long run.\n",
    "\n",
    "#### *FIX DATA TYPES*\n",
    "\n",
    "Counting the number of natural disaster occurrences means handling **discrete variables**, so it would make the most sense to convert all of the values in every DataFrame to integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 128 entries, Afghanistan to Zimbabwe\n",
      "Data columns (total 39 columns):\n",
      "1970    128 non-null int64\n",
      "1971    128 non-null int64\n",
      "1972    128 non-null int64\n",
      "1973    128 non-null int64\n",
      "1974    128 non-null int64\n",
      "1975    128 non-null int64\n",
      "1976    128 non-null int64\n",
      "1977    128 non-null int64\n",
      "1978    128 non-null int64\n",
      "1979    128 non-null int64\n",
      "1980    128 non-null int64\n",
      "1981    128 non-null int64\n",
      "1982    128 non-null int64\n",
      "1983    128 non-null int64\n",
      "1984    128 non-null int64\n",
      "1985    128 non-null int64\n",
      "1986    128 non-null int64\n",
      "1987    128 non-null int64\n",
      "1988    128 non-null int64\n",
      "1989    128 non-null int64\n",
      "1990    128 non-null int64\n",
      "1991    128 non-null int64\n",
      "1992    128 non-null int64\n",
      "1993    128 non-null int64\n",
      "1994    128 non-null int64\n",
      "1995    128 non-null int64\n",
      "1996    128 non-null int64\n",
      "1997    128 non-null int64\n",
      "1998    128 non-null int64\n",
      "1999    128 non-null int64\n",
      "2000    128 non-null int64\n",
      "2001    128 non-null int64\n",
      "2002    128 non-null int64\n",
      "2003    128 non-null int64\n",
      "2004    128 non-null int64\n",
      "2005    128 non-null int64\n",
      "2006    128 non-null int64\n",
      "2007    128 non-null int64\n",
      "2008    128 non-null int64\n",
      "dtypes: int64(39)\n",
      "memory usage: 45.0+ KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 97 entries, Afghanistan to Yugoslavia\n",
      "Data columns (total 39 columns):\n",
      "1970    97 non-null int64\n",
      "1971    97 non-null int64\n",
      "1972    97 non-null int64\n",
      "1973    97 non-null int64\n",
      "1974    97 non-null int64\n",
      "1975    97 non-null int64\n",
      "1976    97 non-null int64\n",
      "1977    97 non-null int64\n",
      "1978    97 non-null int64\n",
      "1979    97 non-null int64\n",
      "1980    97 non-null int64\n",
      "1981    97 non-null int64\n",
      "1982    97 non-null int64\n",
      "1983    97 non-null int64\n",
      "1984    97 non-null int64\n",
      "1985    97 non-null int64\n",
      "1986    97 non-null int64\n",
      "1987    97 non-null int64\n",
      "1988    97 non-null int64\n",
      "1989    97 non-null int64\n",
      "1990    97 non-null int64\n",
      "1991    97 non-null int64\n",
      "1992    97 non-null int64\n",
      "1993    97 non-null int64\n",
      "1994    97 non-null int64\n",
      "1995    97 non-null int64\n",
      "1996    97 non-null int64\n",
      "1997    97 non-null int64\n",
      "1998    97 non-null int64\n",
      "1999    97 non-null int64\n",
      "2000    97 non-null int64\n",
      "2001    97 non-null int64\n",
      "2002    97 non-null int64\n",
      "2003    97 non-null int64\n",
      "2004    97 non-null int64\n",
      "2005    97 non-null int64\n",
      "2006    97 non-null int64\n",
      "2007    97 non-null int64\n",
      "2008    97 non-null int64\n",
      "dtypes: int64(39)\n",
      "memory usage: 32.8+ KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 143 entries, Afghanistan to Zimbabwe\n",
      "Data columns (total 38 columns):\n",
      "1970    143 non-null int64\n",
      "1971    143 non-null int64\n",
      "1972    143 non-null int64\n",
      "1974    143 non-null int64\n",
      "1975    143 non-null int64\n",
      "1976    143 non-null int64\n",
      "1977    143 non-null int64\n",
      "1978    143 non-null int64\n",
      "1979    143 non-null int64\n",
      "1980    143 non-null int64\n",
      "1981    143 non-null int64\n",
      "1982    143 non-null int64\n",
      "1983    143 non-null int64\n",
      "1984    143 non-null int64\n",
      "1985    143 non-null int64\n",
      "1986    143 non-null int64\n",
      "1987    143 non-null int64\n",
      "1988    143 non-null int64\n",
      "1989    143 non-null int64\n",
      "1990    143 non-null int64\n",
      "1991    143 non-null int64\n",
      "1992    143 non-null int64\n",
      "1993    143 non-null int64\n",
      "1994    143 non-null int64\n",
      "1995    143 non-null int64\n",
      "1996    143 non-null int64\n",
      "1997    143 non-null int64\n",
      "1998    143 non-null int64\n",
      "1999    143 non-null int64\n",
      "2000    143 non-null int64\n",
      "2001    143 non-null int64\n",
      "2002    143 non-null int64\n",
      "2003    143 non-null int64\n",
      "2004    143 non-null int64\n",
      "2005    143 non-null int64\n",
      "2006    143 non-null int64\n",
      "2007    143 non-null int64\n",
      "2008    143 non-null int64\n",
      "dtypes: int64(38)\n",
      "memory usage: 48.6+ KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 182 entries, Afghanistan to Zimbabwe\n",
      "Data columns (total 39 columns):\n",
      "1970    182 non-null int64\n",
      "1971    182 non-null int64\n",
      "1972    182 non-null int64\n",
      "1973    182 non-null int64\n",
      "1974    182 non-null int64\n",
      "1975    182 non-null int64\n",
      "1976    182 non-null int64\n",
      "1977    182 non-null int64\n",
      "1978    182 non-null int64\n",
      "1979    182 non-null int64\n",
      "1980    182 non-null int64\n",
      "1981    182 non-null int64\n",
      "1982    182 non-null int64\n",
      "1983    182 non-null int64\n",
      "1984    182 non-null int64\n",
      "1985    182 non-null int64\n",
      "1986    182 non-null int64\n",
      "1987    182 non-null int64\n",
      "1988    182 non-null int64\n",
      "1989    182 non-null int64\n",
      "1990    182 non-null int64\n",
      "1991    182 non-null int64\n",
      "1992    182 non-null int64\n",
      "1993    182 non-null int64\n",
      "1994    182 non-null int64\n",
      "1995    182 non-null int64\n",
      "1996    182 non-null int64\n",
      "1997    182 non-null int64\n",
      "1998    182 non-null int64\n",
      "1999    182 non-null int64\n",
      "2000    182 non-null int64\n",
      "2001    182 non-null int64\n",
      "2002    182 non-null int64\n",
      "2003    182 non-null int64\n",
      "2004    182 non-null int64\n",
      "2005    182 non-null int64\n",
      "2006    182 non-null int64\n",
      "2007    182 non-null int64\n",
      "2008    182 non-null int64\n",
      "dtypes: int64(39)\n",
      "memory usage: 61.9+ KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 181 entries, Afghanistan to Zimbabwe\n",
      "Data columns (total 39 columns):\n",
      "1970    181 non-null int64\n",
      "1971    181 non-null int64\n",
      "1972    181 non-null int64\n",
      "1973    181 non-null int64\n",
      "1974    181 non-null int64\n",
      "1975    181 non-null int64\n",
      "1976    181 non-null int64\n",
      "1977    181 non-null int64\n",
      "1978    181 non-null int64\n",
      "1979    181 non-null int64\n",
      "1980    181 non-null int64\n",
      "1981    181 non-null int64\n",
      "1982    181 non-null int64\n",
      "1983    181 non-null int64\n",
      "1984    181 non-null int64\n",
      "1985    181 non-null int64\n",
      "1986    181 non-null int64\n",
      "1987    181 non-null int64\n",
      "1988    181 non-null int64\n",
      "1989    181 non-null int64\n",
      "1990    181 non-null int64\n",
      "1991    181 non-null int64\n",
      "1992    181 non-null int64\n",
      "1993    181 non-null int64\n",
      "1994    181 non-null int64\n",
      "1995    181 non-null int64\n",
      "1996    181 non-null int64\n",
      "1997    181 non-null int64\n",
      "1998    181 non-null int64\n",
      "1999    181 non-null int64\n",
      "2000    181 non-null int64\n",
      "2001    181 non-null int64\n",
      "2002    181 non-null int64\n",
      "2003    181 non-null int64\n",
      "2004    181 non-null int64\n",
      "2005    181 non-null int64\n",
      "2006    181 non-null int64\n",
      "2007    181 non-null int64\n",
      "2008    181 non-null int64\n",
      "dtypes: int64(39)\n",
      "memory usage: 61.6+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#Display data types of each DataFrame\n",
    "for df in np.ndenumerate(dfs_nd_dropped):\n",
    "    print(df[1].info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the columns contain only integer values, so we do not have to perform any data type conversion or extraction\n",
    "\n",
    "Before generating a final DataFrame containing the sum of each natural disaster occurrence per country and year, **let's first check to see that their year ranges are consistent.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['1970', '1971', '1972', '1973', '1974', '1975', '1976', '1977', '1978',\n",
      "       '1979', '1980', '1981', '1982', '1983', '1984', '1985', '1986', '1987',\n",
      "       '1988', '1989', '1990', '1991', '1992', '1993', '1994', '1995', '1996',\n",
      "       '1997', '1998', '1999', '2000', '2001', '2002', '2003', '2004', '2005',\n",
      "       '2006', '2007', '2008'],\n",
      "      dtype='object')\n",
      "Index(['1970', '1971', '1972', '1973', '1974', '1975', '1976', '1977', '1978',\n",
      "       '1979', '1980', '1981', '1982', '1983', '1984', '1985', '1986', '1987',\n",
      "       '1988', '1989', '1990', '1991', '1992', '1993', '1994', '1995', '1996',\n",
      "       '1997', '1998', '1999', '2000', '2001', '2002', '2003', '2004', '2005',\n",
      "       '2006', '2007', '2008'],\n",
      "      dtype='object')\n",
      "Index(['1970', '1971', '1972', '1974', '1975', '1976', '1977', '1978', '1979',\n",
      "       '1980', '1981', '1982', '1983', '1984', '1985', '1986', '1987', '1988',\n",
      "       '1989', '1990', '1991', '1992', '1993', '1994', '1995', '1996', '1997',\n",
      "       '1998', '1999', '2000', '2001', '2002', '2003', '2004', '2005', '2006',\n",
      "       '2007', '2008'],\n",
      "      dtype='object')\n",
      "Index(['1970', '1971', '1972', '1973', '1974', '1975', '1976', '1977', '1978',\n",
      "       '1979', '1980', '1981', '1982', '1983', '1984', '1985', '1986', '1987',\n",
      "       '1988', '1989', '1990', '1991', '1992', '1993', '1994', '1995', '1996',\n",
      "       '1997', '1998', '1999', '2000', '2001', '2002', '2003', '2004', '2005',\n",
      "       '2006', '2007', '2008'],\n",
      "      dtype='object')\n",
      "Index(['1970', '1971', '1972', '1973', '1974', '1975', '1976', '1977', '1978',\n",
      "       '1979', '1980', '1981', '1982', '1983', '1984', '1985', '1986', '1987',\n",
      "       '1988', '1989', '1990', '1991', '1992', '1993', '1994', '1995', '1996',\n",
      "       '1997', '1998', '1999', '2000', '2001', '2002', '2003', '2004', '2005',\n",
      "       '2006', '2007', '2008'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#Extract year columns from last DataFrame, use as point of comparison\n",
    "column_names = dfs_nd_dropped[-1].columns\n",
    "\n",
    "\n",
    "for i in range(len(dfs_nd_dropped)):\n",
    "    print(dfs_nd_dropped[i].columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 3rd DataFrame does not have the year '1973' but besides that, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### *OUTER MERGE DATASETS*\n",
    "> **Main idea:** We are using an ***outer merge*** because our data implies that each country in each DataFrame satisfies the following conditions:\n",
    "\n",
    "1) contains reliable non-null data\n",
    "\n",
    "2) represents the true number of its respective natural disaster per year\n",
    "\n",
    "> Consider that we are merging df1 and df2.\n",
    "\n",
    "If df1 contains countries that df2 does not AND df2 contains countries that df1 does not, we want all of those countries to show up in the final result.\n",
    "\n",
    "If df2 contains years that df2 does not AND df2 contains years that df1 does not, we want all of those years to show up in the final result anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['1970', '1971', '1972', '1973', '1974', '1975', '1976', '1977', '1978',\n",
      "       '1979', '1980', '1981', '1982', '1983', '1984', '1985', '1986', '1987',\n",
      "       '1988', '1989', '1990', '1991', '1992', '1993', '1994', '1995', '1996',\n",
      "       '1997', '1998', '1999', '2000', '2001', '2002', '2003', '2004', '2005',\n",
      "       '2006', '2007', '2008'],\n",
      "      dtype='object')\n",
      "Index(['1970', '1971', '1972', '1973', '1974', '1975', '1976', '1977', '1978',\n",
      "       '1979', '1980', '1981', '1982', '1983', '1984', '1985', '1986', '1987',\n",
      "       '1988', '1989', '1990', '1991', '1992', '1993', '1994', '1995', '1996',\n",
      "       '1997', '1998', '1999', '2000', '2001', '2002', '2003', '2004', '2005',\n",
      "       '2006', '2007', '2008'],\n",
      "      dtype='object')\n",
      "Index(['1970', '1971', '1972', '1974', '1975', '1976', '1977', '1978', '1979',\n",
      "       '1980', '1981', '1982', '1983', '1984', '1985', '1986', '1987', '1988',\n",
      "       '1989', '1990', '1991', '1992', '1993', '1994', '1995', '1996', '1997',\n",
      "       '1998', '1999', '2000', '2001', '2002', '2003', '2004', '2005', '2006',\n",
      "       '2007', '2008'],\n",
      "      dtype='object')\n",
      "Index(['1970', '1971', '1972', '1973', '1974', '1975', '1976', '1977', '1978',\n",
      "       '1979', '1980', '1981', '1982', '1983', '1984', '1985', '1986', '1987',\n",
      "       '1988', '1989', '1990', '1991', '1992', '1993', '1994', '1995', '1996',\n",
      "       '1997', '1998', '1999', '2000', '2001', '2002', '2003', '2004', '2005',\n",
      "       '2006', '2007', '2008'],\n",
      "      dtype='object')\n",
      "Index(['1970', '1971', '1972', '1973', '1974', '1975', '1976', '1977', '1978',\n",
      "       '1979', '1980', '1981', '1982', '1983', '1984', '1985', '1986', '1987',\n",
      "       '1988', '1989', '1990', '1991', '1992', '1993', '1994', '1995', '1996',\n",
      "       '1997', '1998', '1999', '2000', '2001', '2002', '2003', '2004', '2005',\n",
      "       '2006', '2007', '2008'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *CLASSIFY FREQUENCY AS 'LOW', 'MEDIUM', OR 'HIGH'*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obesity Rates\n",
    "> Though generally it may seem that a higher BMI indicates a 'healthier' weight for male, [this article](https://signup.weightwatchers.co.uk/util/art/index_art.aspx?art_id=31901&tabnum=1&sc=803&subnav=Science+Library%3A+Health+and+Weight) clarifies that BMI rates above 25 would still indicate ill health: an equal expectation for both men and women. Based on this fast, I decided it was safe to generate a new CSV, 'bmi_indicator,' for both men and women, by filling each slot with the mean BMI value for men and women."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating 'natural_disasters_killed.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='eda'></a>\n",
    "# III) Exploring the Data\n",
    "\n",
    "> **Tip**: Now that you've trimmed and cleaned your data, you're ready to move on to exploration. Compute statistics and create visualizations with the goal of addressing the research questions that you posed in the Introduction section. It is recommended that you be systematic with your approach. Look at one variable at a time, and then follow it up by looking at relationships between variables.\n",
    "\n",
    "### How much have global obesity rates changed in countries with high forest area vs. countries with low forest area?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use this, and more code cells, to explore your data. Don't forget to add\n",
    "#   Markdown cells to document your observations and findings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Does a country’s likelihood of experiencing a natural disaster affect the homicidal tendencies of its citizens?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Continue to explore the data to address your additional research\n",
    "#   questions. Add more headers as needed if you have more questions to\n",
    "#   investigate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='conclusions'></a>\n",
    "# IV) Conclusions\n",
    "\n",
    "> **Tip**: Finally, summarize your findings and the results that have been performed. Make sure that you are clear with regards to the limitations of your exploration. If you haven't done any statistical tests, do not imply any statistical conclusions. And make sure you avoid implying causation from correlation!\n",
    "\n",
    "> **Tip**: Once you are satisfied with your work here, check over your report to make sure that it is satisfies all the areas of the rubric (found on the project submission page at the end of the lesson). You should also probably remove all of the \"Tips\" like this one so that the presentation is as polished as possible.\n",
    "\n",
    "## Submitting your Project \n",
    "\n",
    "> Before you submit your project, you need to create a .html or .pdf version of this notebook in the workspace here. To do that, run the code cell below. If it worked correctly, you should get a return code of 0, and you should see the generated .html file in the workspace directory (click on the orange Jupyter icon in the upper left).\n",
    "\n",
    "> Alternatively, you can download this report as .html via the **File** > **Download as** submenu, and then manually upload it into the workspace directory by clicking on the orange Jupyter icon in the upper left, then using the Upload button.\n",
    "\n",
    "> Once you've done this, you can submit your project by clicking on the \"Submit Project\" button in the lower right here. This will create and submit a zip file with this .ipynb doc and the .html or .pdf version you created. Congratulations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from subprocess import call\n",
    "call(['python', '-m', 'nbconvert', 'Investigate_a_Dataset.ipynb'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
